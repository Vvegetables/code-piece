#coding=utf-8
from html.parser import HTMLParser
import random
import re

import pymysql
import spacy
from spacy.tokens import Span  # å‡çš„æŠ¥é”™


def test1():
    nlp = spacy.load('en_core_web_sm') #è‹±è¯­
    doc = nlp("Hello, world. Here are two sentences.")
    
    print([t.text for t in doc])
    
    nlp_de = spacy.load('de_core_news_sm') #å¾·è¯­
    doc_de = nlp_de('ich bin ein berliner')
    
    print([t.text for t in doc_de])

#è·å–å•è¯ï¼Œåè¯å—å’Œå¥å­
def test2():
    nlp = spacy.load('en_core_web_sm')
    #æ¢è¡Œå­—ç¬¦ä¸²å†™æ³•ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
    doc = nlp(u"Peach emoji is where it has always been. Peach is the superior "
              u"emoji. It's outranking eggplant ğŸ‘ ")
    print(doc[0].text)          # Peach
    print(doc[1].text)          # emoji
    print(doc[-1].text)         # ğŸ‘
    print(doc[17:19].text)      # outranking eggplant
    
    noun_chunks = list(doc.noun_chunks) #åè¯å¿«
    print(noun_chunks[0].text)  # Peach emoji
    
    sentences = list(doc.sents) #å¥å­
    assert len(sentences) == 3
    print(sentences[1].text)    # 'Peach is the superior emoji.'

#è·å–è¯æ€§æ ‡ç­¾å’Œæ ‡å¿—
def test3():
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')
    apple = doc[0]
    print('Fine-grained POS tag', apple.pos_, apple.pos) #ç»†é¢—ç²’åº¦çš„æ ‡ç­¾
    print('Coarse-grained POS tag', apple.tag_, apple.tag) #ç²—é¢—ç²’åº¦çš„æ ‡ç­¾
    print('Word shape', apple.shape_, apple.shape) #å­—å½¢
    print('Alphanumeric characters?', apple.is_alpha) #æ˜¯å¦æ˜¯å­—æ¯
    print('Punctuation mark?', apple.is_punct) #æ˜¯å¦æ˜¯æ ‡ç‚¹ç¬¦å·
    
    billion = doc[10]
    print('Digit?', billion.is_digit) #æ˜¯å¦æ˜¯æ•°å­—
    print('Like a number?', billion.like_num) #æ„æ€æ˜¯å¦åƒæ•°å­—
    print('Like an email address?', billion.like_email) #æ˜¯å¦åƒé‚®ç®±
    

#å¯¹ä»»ä½•å­—ç¬¦ä¸²ä½¿ç”¨å“ˆå¸Œå€¼
def test4():
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(u'I love coffee')
    
    coffee_hash = nlp.vocab.strings[u'coffee']  # 3197928453018144401
    coffee_text = nlp.vocab.strings[coffee_hash]  # 'coffee'
    print(coffee_hash, coffee_text)
    print(doc[2].orth, coffee_hash)  # 3197928453018144401
    print(doc[2].text, coffee_text)  # 'coffee'
    
    beer_hash = doc.vocab.strings.add(u'beer')  # 3073001599257881079
    beer_text = doc.vocab.strings[beer_hash]  # 'beer'
    print(beer_hash, beer_text)
    
    unicorn_hash = doc.vocab.strings.add(u'ğŸ¦„ ')  # 18234233413267120783
    unicorn_text = doc.vocab.strings[unicorn_hash]  # 'ğŸ¦„ '
    print(unicorn_hash, unicorn_text)

#è¯†åˆ«å¹¶æ›´æ–°å‘½åå®ä½“
def test5():
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(u'San Francisco considers banning sidewalk delivery robots')
    for ent in doc.ents:
        print(ent.text, ent.start_char, ent.end_char, ent.label_)
    
    doc = nlp(u'FB is hiring a new VP of global policy')
    #æ‰‹åŠ¨è®¾ç½®å®ä½“
    doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u'ORG'])]
    for ent in doc.ents:
        print(ent.text, ent.start_char, ent.end_char, ent.label_)


#è®­ç»ƒå’Œæ›´æ–°ç¥ç»ç½‘ç»œæ¨¡å‹
def test6():
    nlp = spacy.load('en')
    train_data = [("Uber blew through $1 million", {'entities': [(0, 4, 'ORG')]})]
    
    with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):
        optimizer = nlp.begin_training()
        for i in range(10):
            random.shuffle(train_data)
            for text, annotations in train_data:
                nlp.update([text], [annotations], sgd=optimizer)
    nlp.to_disk('.\model')
    
def config_db(host,username,password,dbname,port=3306):
    db = pymysql.connect(host,username,password,dbname,port)
#     db = pymysql.connect("127.0.0.1","root","","indextest")
    return db
      
    
#ä½¿ç”¨spacyçš„åè¯å»å®éªŒæ‹†åˆ†æ•ˆæœ
def data_test():
    try:
        nlp = spacy.load('en')
        
        db = config_db("127.0.0.1","root","","indextest")
        cursor = db.cursor(cursor=pymysql.cursors.DictCursor)
        
        sql = " SELECT `id`,`summary`,`submission_year` FROM `australia_data`; "
        cursor.execute(sql)
        rows = cursor.fetchall()
        
        values = []
        for r in rows:
            if r["summary"]:
                doc = nlp(r["summary"])
                myset = set(map(lambda x:str(x).strip().lower(),doc.noun_chunks))
                for m in myset:
                    values.append([m,r["id"],r['submission_year']])
        
        sql = " INSERT INTO `australia_test_index`(`keyname`,`relatedid`,`year`) VALUES(%s,%s,%s); "
        cursor.executemany(sql,values)
        db.commit()
        
        sql = " SELECT `id`,`summary`,`year` FROM `australia_fellowship_data`; "
        cursor.execute(sql)
        rows = cursor.fetchall()
        
        values = []
        for r in rows:
            if r["summary"]:
                doc = nlp(r["summary"])
                myset = set(map(lambda x:str(x).strip().lower(),doc.noun_chunks))
                for m in myset:
                    values.append([m,r["id"],r['year']])
        
        sql = " INSERT INTO `australia_test_fellow_index`(`keyname`,`relatedid`,`year`) VALUES(%s,%s,%s); "
        cursor.executemany(sql,values)
        db.commit()
    except Exception as e:
        print(e)
    
if __name__ == "__main__":
    data_test()
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    